<html>
  <head>
    <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}}); </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <style>
      body {
        font-size: 2em;
        padding: 20px 60px 20px 60px;
      }
    </style
  </head>
  <body>
    <h2>Linear Regression</h3>
    <h4>Hypothesis</h4>
    <div class="formula">
      \(h_\theta(x) = \theta^Tx = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n\)
    </div>
    <h4>Cost Function</h4>
    <div class="formula">
      \(J(\theta) = \frac1{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\)
    </div>
    <div class="formula">
      \(J(\theta) = \frac1{2m}\left[\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 + \lambda \displaystyle \sum_{j=1}^n\theta_j^2\right]\)
    </div>
    <h4>Gradient Descent</h4>
    <div class="formula">
      \(\theta_j := \theta_j - \alpha\frac1{m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\)
    </div>
    <div class="formula">
      \(\theta_j := \theta_j(1 - \alpha \frac \lambda m) - \alpha\frac1{m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\) | \((1 - \alpha \frac \lambda m \lt 1)\)
    </div>
    <h4>Normal Equation</h4>
    <div class="formula">
      \(\theta = (X^tX)^{-1}X^Ty\)
    </div>
    <div class="formula">
      \(\theta = (X^tX + \lambda \begin{bmatrix} 0 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \end{bmatrix})^{-1}X^Ty\)
    </div>
    <h4>Mean Normalization</h4>
    <div class="formula">
      \(\frac{x_i - \mu_i}{max(x_i) - min(x_i)} | \mu = mean\)
    </div>
    <h2>Logistic Regression</h3>
    <h4>Hypothesis</h4>
    <div class="formula">
      \(h_\theta(x) = g(\theta^Tx) = \frac1{1 + e^{-\theta^tx}}\)
    </div>
    <h4>Cost Function</h4>
    <div class="formula">
      \(J(\theta) = -\frac1m\left[\displaystyle\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)}) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))\right]\)
    </div>
    <div class="formula">
      \(J(\theta) = -\frac1m\left[\displaystyle\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)}) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))\right] + \frac \lambda {2m} \displaystyle\sum_{j=1}^n \theta_j^2\)
    </div>
    <h4>Gradient Descent (identical to linear regression)</h4>
    <div class="formula">
      \(\theta_j := \theta_j - \alpha\frac1{m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\)
    </div>
    <h4>One-vs-all</h4>
    <div class="formula">
      \(max(i)h_\theta^{(i)}(x)\)
    </div>
  </body>
</html>
